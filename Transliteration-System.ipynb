{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cmn5s8Nt42B8"
      },
      "source": [
        "Imports and Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ddmxAig4quf"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth evaluate jiwer rouge-score sacrebleu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvG4ULCf50x8"
      },
      "source": [
        "Phonetic Noise Augmentation Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzAK1YvM4wwd"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from unsloth.trainer import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "PHONETIC_SWAPS = {\n",
        "    \"v\": [\"w\"], \"w\": [\"v\"], \"z\": [\"j\"], \"j\": [\"z\"], \"f\": [\"ph\"], \"ph\": [\"f\"],\n",
        "    \"ee\": [\"i\"], \"i\": [\"ee\"], \"oo\": [\"u\"], \"u\": [\"oo\"], \"s\": [\"sh\"], \"sh\": [\"s\"], \"y\": [\"i\"]\n",
        "}\n",
        "SLANG_MAP = {\"hai\": [\"h\", \"ey\"], \"kya\": [\"ky\"], \"nahi\": [\"nhi\", \"nai\"], \"raha\": [\"rha\"]}\n",
        "\n",
        "def inject_phonetic_noise(text, p=0.3):\n",
        "\n",
        "    if not isinstance(text, str): return text\n",
        "    words = text.split()\n",
        "    augmented_words = []\n",
        "\n",
        "    for w in words:\n",
        "        low_word = w.lower()\n",
        "\n",
        "        # 1. Noise Mapping\n",
        "        if low_word in SLANG_MAP and random.random() < p:\n",
        "            low_word = random.choice(SLANG_MAP[low_word])\n",
        "\n",
        "        # 2. Phonetic Swaps\n",
        "        for char, variants in PHONETIC_SWAPS.items():\n",
        "            if char in low_word and random.random() < 0.2:\n",
        "                low_word = low_word.replace(char, random.choice(variants))\n",
        "\n",
        "        # 3. Middle Vowel Drop\n",
        "        if len(low_word) > 4 and random.random() < 0.15:\n",
        "            vowels = re.findall(r\"[aeiou]\", low_word[1:-1])\n",
        "            if vowels:\n",
        "                low_word = low_word.replace(random.choice(vowels), \"\", 1)\n",
        "\n",
        "        # 4. Entity Capitalization Noise (ADDED)\n",
        "        # Randomly capitalize to train model on proper nouns and varied user inputs\n",
        "        if random.random() < 0.2:\n",
        "            low_word = low_word.capitalize()\n",
        "        elif random.random() < 0.05:\n",
        "            low_word = low_word.upper()\n",
        "\n",
        "        augmented_words.append(low_word)\n",
        "\n",
        "    return \" \".join(augmented_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQx0LLv158RD"
      },
      "source": [
        "Data Loading & Complex Character Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4Z49ZuM53Ss"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "ds = load_dataset(\"sk-community/romanized_hindi\", split=\"train\")\n",
        "df = ds.to_pandas().head(3000)\n",
        "\n",
        "def clean_and_normalize(row):\n",
        "    if not isinstance(row[\"Hindi\"], str) or not isinstance(row[\"Transliterated Hindi\"], str):\n",
        "        return None\n",
        "    # NFC merges decomposed characters (critical for half-letters like)\n",
        "    hindi = unicodedata.normalize('NFC', row[\"Hindi\"].strip())\n",
        "    hindi = re.sub(r\"[^\\u0900-\\u097F0-9\\sред!?]\", \"\", hindi)\n",
        "    roman = row[\"Transliterated Hindi\"].strip()\n",
        "    return {\"roman\": roman, \"hindi\": hindi}\n",
        "\n",
        "df = df.apply(clean_and_normalize, axis=1).dropna().apply(pd.Series).reset_index(drop=True)\n",
        "\n",
        "# Generate multi-variants to increase dataset scale and entity robustness\n",
        "df_noisy = df.copy()\n",
        "df_noisy['roman'] = df_noisy['roman'].apply(lambda x: inject_phonetic_noise(x))\n",
        "df_final = pd.concat([df, df_noisy]).drop_duplicates().reset_index(drop=True)\n",
        "print(f\"Dataset expanded to {len(df_final)} rows.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MFpMtQg7nyN"
      },
      "source": [
        "Model Setup and Phonetic Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn6HJWHf6Ag4"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "BASE_MODEL = \"google/gemma-2b-it\"\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    max_seq_length=512,\n",
        "    load_in_4bit=True\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model, r=128, lora_alpha=128,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
        ")\n",
        "\n",
        "def tokenize(batch):\n",
        "    input_ids, labels = [], []\n",
        "    for r, h in zip(batch[\"roman\"], batch[\"hindi\"]):\n",
        "        prompt_text = f\"Hinglish: {r}\\nHindi: \"\n",
        "        full_text = prompt_text + h + tokenizer.eos_token\n",
        "\n",
        "        tokenized_full = tokenizer(full_text, truncation=True, max_length=128)\n",
        "        tokenized_prompt = tokenizer(prompt_text, truncation=True, max_length=128, add_special_tokens=False)\n",
        "\n",
        "        prompt_len = len(tokenized_prompt[\"input_ids\"])\n",
        "        # Mask prompt so model only learns to predict Devanagari sequence\n",
        "        label = [-100] * prompt_len + tokenized_full[\"input_ids\"][prompt_len:]\n",
        "\n",
        "        input_ids.append(tokenized_full[\"input_ids\"])\n",
        "        labels.append(label)\n",
        "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
        "\n",
        "# Prepare final training data\n",
        "dataset = Dataset.from_pandas(df_final).shuffle(seed=42)\n",
        "split = dataset.train_test_split(test_size=0.1)\n",
        "train_ds = split[\"train\"].map(tokenize, batched=True, remove_columns=dataset.column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsUZNhge85p6"
      },
      "source": [
        "Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Pn0AuvD7q0x"
      },
      "outputs": [],
      "source": [
        "from unsloth.trainer import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "# Trainer setup\n",
        "trainer = SFTTrainer(\n",
        "    model=model, tokenizer=tokenizer, train_dataset=train_ds,\n",
        "    args=TrainingArguments(\n",
        "        output_dir=\"./hinglish_lora\", per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4, learning_rate=5e-5, num_train_epochs=2,\n",
        "        fp16=True, logging_steps=50, optim=\"adamw_8bit\", report_to=\"none\"\n",
        "    ),\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xoIgt4iUDXD3"
      },
      "source": [
        "Metric and Evaluation Utility Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "npeYJlV0nIPB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def generate_text_fixed(roman):\n",
        "\n",
        "    # Force lowercase as models often handle standardized case better for phonetics\n",
        "    prompt = f\"Hinglish: {roman.lower()}\\nHindi: \"\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=128,\n",
        "            do_sample=False,\n",
        "            repetition_penalty=1.2,\n",
        "            no_repeat_ngram_size=3,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    # Extract newly generated tokens only\n",
        "    input_len = inputs.input_ids.shape[1]\n",
        "    decoded = tokenizer.decode(out[0][input_len:], skip_special_tokens=True).strip()\n",
        "    return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwba8b6k89fL"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import unicodedata\n",
        "\n",
        "def compute_all_metrics(preds, refs):\n",
        "\n",
        "\n",
        "    # 1. Standard character and sequence metrics\n",
        "    cer = evaluate.load(\"cer\").compute(predictions=preds, references=refs)\n",
        "    wer = evaluate.load(\"wer\").compute(predictions=preds, references=refs)\n",
        "    chrf = evaluate.load(\"chrf\").compute(predictions=preds, references=refs)[\"score\"]\n",
        "\n",
        "    # 2. FLATTEN sentences into words for Word-Level Accuracy\n",
        "    all_pred_words = []\n",
        "    all_ref_words = []\n",
        "\n",
        "    for p_sent, r_sent in zip(preds, refs):\n",
        "        # Splitting sentences into individual words\n",
        "        p_words = p_sent.split()\n",
        "        r_words = r_sent.split()\n",
        "\n",
        "        # Alignment padding: Ensure we compare equal lengths\n",
        "        max_len = max(len(p_words), len(r_words))\n",
        "        p_words += [\"<PAD>\"] * (max_len - len(p_words))\n",
        "        r_words += [\"<PAD>\"] * (max_len - len(r_words))\n",
        "\n",
        "        all_pred_words.extend(p_words)\n",
        "        all_ref_words.extend(r_words)\n",
        "\n",
        "    # 3. Calculate binary success for every individual word\n",
        "    y_true = [1] * len(all_ref_words)\n",
        "    y_pred = [1 if p == r else 0 for p, r in zip(all_pred_words, all_ref_words)]\n",
        "\n",
        "    # Compute Word-Level metrics\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average='binary', zero_division=0\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"CER\": cer,\n",
        "        \"WER\": wer,\n",
        "        \"chrF_Score\": chrf,\n",
        "        \"Model_Word_Precision\": prec,\n",
        "        \"Model_Word_Recall\": rec,\n",
        "        \"Model_Word_F1\": f1\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUxvQr16DtV_"
      },
      "source": [
        "Evaluation on Internal Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmhlCux5Duh5"
      },
      "outputs": [],
      "source": [
        "print(\"--- Internal Dataset Evaluation (Word-Level Recall/F1) ---\")\n",
        "\n",
        "# Ensure evaluation subset is selected\n",
        "eval_subset_internal = split[\"test\"].shuffle(seed=42).select(range(min(50, len(split[\"test\"]))))\n",
        "\n",
        "preds_int, refs_int = [], []\n",
        "for ex in tqdm(eval_subset_internal, desc=\"Internal Eval\"):\n",
        "    # Using your fixed prediction logic (Greedy + Repetition Penalty)\n",
        "    p = generate_text_fixed(ex[\"roman\"])\n",
        "    # Normalize to NFC to ensure character clusters match correctly\n",
        "    r = unicodedata.normalize('NFC', ex[\"hindi\"].strip())\n",
        "\n",
        "    preds_int.append(p)\n",
        "    refs_int.append(r)\n",
        "\n",
        "# Compute new Model Recall and F1 metrics\n",
        "internal_results = compute_all_metrics(preds_int, refs_int)\n",
        "\n",
        "for metric, value in internal_results.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFykeUKIEwEW"
      },
      "source": [
        "Evaluation on External codebyam Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7aboWsPDxnq"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "import unicodedata\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(\"\\n--- External Dataset Evaluation (codebyam - 50 rows) ---\")\n",
        "\n",
        "# Load the external dataset\n",
        "ext_ds_full = load_dataset(\"codebyam/Hinglish-Hindi-Transliteration-Dataset\", split=\"train\")\n",
        "\n",
        "# Select a subset of 50 rows for evaluation\n",
        "eval_subset_ext = ext_ds_full.shuffle(seed=42).select(range(50))\n",
        "\n",
        "preds_ext, refs_ext = [], []\n",
        "\n",
        "for ex in tqdm(eval_subset_ext, desc=\"External Eval\"):\n",
        "    # Generate transliteration using your fixed prediction function\n",
        "    # (Greedy search + repetition penalty for high precision)\n",
        "    p = generate_text_fixed(ex[\"Hinglish\"])\n",
        "\n",
        "    # Normalize Devanagari to NFC to ensure complex phonetic clusters\n",
        "    # (like half-characters) are compared accurately\n",
        "    r = unicodedata.normalize('NFC', ex[\"Hindi\"].strip())\n",
        "\n",
        "    preds_ext.append(p)\n",
        "    refs_ext.append(r)\n",
        "\n",
        "# Compute word-level metrics: CER, WER, chrF, Model Recall, and Model F1\n",
        "external_results = compute_all_metrics(preds_ext, refs_ext)\n",
        "\n",
        "for metric, value in external_results.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Samples"
      ],
      "metadata": {
        "id": "nLXUWCfFo_JY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11b60a94"
      },
      "outputs": [],
      "source": [
        "print(\"--- Testing on 5 Samples from External Dataset (codebyam) ---\")\n",
        "\n",
        "# Create a subset of 5 samples from the external dataset\n",
        "eval_subset_ext_5_samples = ext_ds_full.shuffle(seed=42).select(range(min(5, len(ext_ds_full))))\n",
        "\n",
        "preds_ext_5_samples, refs_ext_5_samples = [], []\n",
        "\n",
        "for i, ex in enumerate(eval_subset_ext_5_samples):\n",
        "    # Generate text using sampling logic\n",
        "    p = generate_text_fixed(ex[\"Hinglish\"])\n",
        "    # Normalize external references for a fair comparison of phonetic clusters\n",
        "    r = unicodedata.normalize('NFC', ex[\"Hindi\"].strip())\n",
        "\n",
        "    preds_ext_5_samples.append(p)\n",
        "    refs_ext_5_samples.append(r)\n",
        "\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Roman (Input): {ex['Hinglish']}\")\n",
        "    print(f\"Predicted Hindi: {p}\")\n",
        "    print(f\"Actual Hindi:    {r}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ae39a7f7"
      },
      "outputs": [],
      "source": [
        "print(\"--- Testing on 5 Samples from Internal Test Split ---\")\n",
        "\n",
        "# Create a subset of 5 samples from the internal test split\n",
        "eval_subset_5_samples = split[\"test\"].shuffle(seed=42).select(range(min(5, len(split[\"test\"]))))\n",
        "\n",
        "preds_5_samples, refs_5_samples = [], []\n",
        "\n",
        "for i, ex in enumerate(eval_subset_5_samples):\n",
        "    # Generate text using sampling logic\n",
        "    p = generate_text_fixed(ex[\"roman\"])\n",
        "    # Normalize Devanagari for consistent character clustering\n",
        "    r = unicodedata.normalize('NFC', ex[\"hindi\"].strip())\n",
        "\n",
        "    preds_5_samples.append(p)\n",
        "    refs_5_samples.append(r)\n",
        "\n",
        "    print(f\"\\nSample {i+1}:\")\n",
        "    print(f\"Roman (Input): {ex['roman']}\")\n",
        "    print(f\"Predicted Hindi: {p}\")\n",
        "    print(f\"Actual Hindi:    {r}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance Comparison"
      ],
      "metadata": {
        "id": "SDS6enWvtqPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the metrics to be plotted\n",
        "plot_metrics = [\n",
        "    \"CER\\n(Lower is Better)\",\n",
        "    \"WER\\n(Lower is Better)\",\n",
        "    \"chrF / 100\\n(Higher is Better)\",\n",
        "    \"Word Recall\\n(Higher is Better)\",\n",
        "    \"Word F1\\n(Higher is Better)\"\n",
        "]\n",
        "\n",
        "# Real-time data extraction from your evaluation dictionaries\n",
        "# This will automatically update whenever you re-run your evaluation blocks\n",
        "try:\n",
        "    internal_plot_values = [\n",
        "        internal_results[\"CER\"],\n",
        "        internal_results[\"WER\"],\n",
        "        internal_results[\"chrF_Score\"] / 100, # Normalized to 0.0-1.0 scale\n",
        "        internal_results[\"Model_Word_Recall\"],\n",
        "        internal_results[\"Model_Word_F1\"]\n",
        "    ]\n",
        "\n",
        "    external_plot_values = [\n",
        "        external_results[\"CER\"],\n",
        "        external_results[\"WER\"],\n",
        "        external_results[\"chrF_Score\"] / 100, # Normalized to 0.0-1.0 scale\n",
        "        external_results[\"Model_Word_Recall\"],\n",
        "        external_results[\"Model_Word_F1\"]\n",
        "    ]\n",
        "except KeyError as e:\n",
        "    print(f\"Error: Metric {e} not found. Ensure compute_all_metrics has run for both datasets.\")\n",
        "    internal_plot_values = [0]*5\n",
        "    external_plot_values = [0]*5\n",
        "\n",
        "x = np.arange(len(plot_metrics))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "rects1 = ax.bar(x - width/2, internal_plot_values, width, label='Internal Dataset (sk-community/romanized_hindi)', color='#4C72B0')\n",
        "rects2 = ax.bar(x + width/2, external_plot_values, width, label='External Dataset (codebyam/Hinglish-Hindi-Transliteration-Dataset)', color='#9BBBEA')\n",
        "\n",
        "# Styling and dynamic labeling\n",
        "ax.set_ylabel('Metric Score (Normalized 0.0 - 1.0)')\n",
        "ax.set_title('Performance Comparison ', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(plot_metrics, fontsize=10)\n",
        "ax.legend()\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Dynamic value labels on top of bars\n",
        "def autolabel(rects):\n",
        "    for rect in rects:\n",
        "        height = rect.get_height()\n",
        "        ax.annotate(f'{height:.4f}',\n",
        "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
        "                    xytext=(0, 3),\n",
        "                    textcoords=\"offset points\",\n",
        "                    ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "autolabel(rects1)\n",
        "autolabel(rects2)\n",
        "\n",
        "plt.ylim(0, 1.1) # Limits set for visibility of top-labels\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TndjwquHlNVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "77b5XCfUqGZr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
